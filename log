Sender: LSF System <lsfadmin@host089.jc.rl.ac.uk>
Subject: Job 6711415: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py> in cluster <lotus> Exited

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 16:08:01 2017.
Job was executed on host(s) <host089.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 16:40:00 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 16:40:00 2017.
Terminated at Thu Oct  5 16:40:01 2017.
Results reported at Thu Oct  5 16:40:01 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.27 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   1 sec.
    Turnaround time :                            1920 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 2047
  daemon proc 2050 on host 192.168.103.89
    rank 0:  proc 2054
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.103.89 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 41, in main
    comm = mpi4py.COMM_WORLD
NameError: global name 'mpi4py' is not defined
Sender: LSF System <lsfadmin@host088.jc.rl.ac.uk>
Subject: Job 6713795: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py> in cluster <lotus> Exited

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 16:43:08 2017.
Job was executed on host(s) <host088.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 16:43:08 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 16:43:08 2017.
Terminated at Thu Oct  5 16:43:10 2017.
Results reported at Thu Oct  5 16:43:10 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.27 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   2 sec.
    Turnaround time :                            2 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 13502
  daemon proc 13505 on host 192.168.103.88
    rank 0:  proc 13509
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.103.88 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 45, in main
    setup_config_file = sys.argv[1]
IndexError: list index out of range
Sender: LSF System <lsfadmin@host088.jc.rl.ac.uk>
Subject: Job 6713796: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Exited

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 16:43:30 2017.
Job was executed on host(s) <host088.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 16:43:31 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 16:43:31 2017.
Terminated at Thu Oct  5 16:43:32 2017.
Results reported at Thu Oct  5 16:43:32 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.26 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   1 sec.
    Turnaround time :                            2 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 13544
  daemon proc 13547 on host 192.168.103.88
    rank 0:  proc 13551
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.103.88 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 46, in main
    setup = get_setup(setup_config_file)
  File "/home/users/mjones07/userbased-bench/main.py", line 34, in get_setup
    setup[line.split('=')[0]] = line.split(':')[1].strip()
IndexError: list index out of range
Sender: LSF System <lsfadmin@host088.jc.rl.ac.uk>
Subject: Job 6713797: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Done

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 16:44:05 2017.
Job was executed on host(s) <host088.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 16:44:05 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 16:44:05 2017.
Terminated at Thu Oct  5 16:44:14 2017.
Results reported at Thu Oct  5 16:44:14 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1.34 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   9 sec.
    Turnaround time :                            9 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 13603
  daemon proc 13606 on host 192.168.103.88
    rank 0:  proc 13610
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.103.88 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

256000000,256000000,1000000.0,256.0,sequential,0.19,0.186105966568,1375.56041174
Sender: LSF System <lsfadmin@host121.jc.rl.ac.uk>
Subject: Job 6715043: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Exited

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 16:46:09 2017.
Job was executed on host(s) <host121.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 16:46:09 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 16:46:09 2017.
Terminated at Thu Oct  5 16:46:12 2017.
Results reported at Thu Oct  5 16:46:12 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1.12 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   3 sec.
    Turnaround time :                            3 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 20764
  daemon proc 20767 on host 192.168.106.121
    rank 0:  proc 20771
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.106.121 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 58, in main
    results = readfile(rank, fid+'.nc', setup['readpattern'], setup['buffersize'], setup['randcount'])
  File "/home/users/mjones07/userbased-bench/readfile_bin.py", line 81, in readfile_1d
    results = seq_read_1d(fid, num_elements)
TypeError: seq_read_1d() takes exactly 3 arguments (2 given)
Sender: LSF System <lsfadmin@host121.jc.rl.ac.uk>
Subject: Job 6715052: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Done

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 16:46:43 2017.
Job was executed on host(s) <host121.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 16:46:43 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 16:46:43 2017.
Terminated at Thu Oct  5 16:46:47 2017.
Results reported at Thu Oct  5 16:46:47 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1.35 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   4 sec.
    Turnaround time :                            4 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 20987
  daemon proc 20990 on host 192.168.106.121
    rank 0:  proc 20994
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.106.121 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

256000000,256000000,1000000.0,256.0,sequential,0.18,0.19450211525,1316.18105886
Sender: LSF System <lsfadmin@host150.jc.rl.ac.uk>
Subject: Job 6715056: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Exited

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 16:47:42 2017.
Job was executed on host(s) <4*host150.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 16:48:44 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 16:48:44 2017.
Terminated at Thu Oct  5 16:48:48 2017.
Results reported at Thu Oct  5 16:48:48 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   10.56 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   4 sec.
    Turnaround time :                            66 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 23963
  daemon proc 23966 on host 192.168.109.150
    rank 0:  proc 23970
    rank 1:  proc 23971
    rank 2:  proc 23972
    rank 3:  proc 23973
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:2: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:3: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:1: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.109.150 -- ranks 0 - 3

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 58, in main
    results = readfile(rank, fid+'.nc', setup['readpattern'], setup['buffersize'], setup['randcount'])
  File "/home/users/mjones07/userbased-bench/readfile_bin.py", line 81, in readfile_1d
    results = seq_read_1d(mpirank, fid, num_elements)
  File "/home/users/mjones07/userbased-bench/readfile_bin.py", line 10, in seq_read_1d
    f = Dataset(fid,'r')
  File "netCDF4/_netCDF4.pyx", line 1875, in netCDF4._netCDF4.Dataset.__init__ (netCDF4/_netCDF4.c:13376)
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 58, in main
    results = readfile(rank, fid+'.nc', setup['readpattern'], setup['buffersize'], setup['randcount'])
  File "/home/users/mjones07/userbased-bench/readfile_bin.py", line 81, in readfile_1d
    results = seq_read_1d(mpirank, fid, num_elements)
  File "/home/users/mjones07/userbased-bench/readfile_bin.py", line 10, in seq_read_1d
    f = Dataset(fid,'r')
  File "netCDF4/_netCDF4.pyx", line 1875, in netCDF4._netCDF4.Dataset.__init__ (netCDF4/_netCDF4.c:13376)
  File "netCDF4/_netCDF4.pyx", line 1581, in netCDF4._netCDF4._ensure_nc_success (netCDF4/_netCDF4.c:12162)
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 58, in main
    results = readfile(rank, fid+'.nc', setup['readpattern'], setup['buffersize'], setup['randcount'])
  File "/home/users/mjones07/userbased-bench/readfile_bin.py", line 81, in readfile_1d
    results = seq_read_1d(mpirank, fid, num_elements)
  File "/home/users/mjones07/userbased-bench/readfile_bin.py", line 10, in seq_read_1d
    f = Dataset(fid,'r')
  File "netCDF4/_netCDF4.pyx", line 1875, in netCDF4._netCDF4.Dataset.__init__ (netCDF4/_netCDF4.c:13376)
  File "netCDF4/_netCDF4.pyx", line 1581, in netCDF4._netCDF4._ensure_nc_success (netCDF4/_netCDF4.c:12162)
IOError: NetCDF: Unknown file format
  File "netCDF4/_netCDF4.pyx", line 1581, in netCDF4._netCDF4._ensure_nc_success (netCDF4/_netCDF4.c:12162)
IOError: NetCDF: Unknown file format
IOError: NetCDF: Unknown file format
0,256000000,256000000,1000000.0,256.0,sequential,0.18,0.180833101273,1415.67001947
Sender: LSF System <lsfadmin@host199.jc.rl.ac.uk>
Subject: Job 6715783: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Exited

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 16:55:11 2017.
Job was executed on host(s) <4*host199.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 16:58:42 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 16:58:42 2017.
Terminated at Thu Oct  5 16:58:48 2017.
Results reported at Thu Oct  5 16:58:48 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   8.69 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   6 sec.
    Turnaround time :                            217 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 31889
  daemon proc 31892 on host 192.168.113.199
    rank 0:  proc 31896
    rank 1:  proc 31897
    rank 2:  proc 31898
    rank 3:  proc 31899
main.py: Rank 0:1: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:2: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:3: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.113.199 -- ranks 0 - 3

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
    main()
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 58, in main
  File "/home/users/mjones07/userbased-bench/main.py", line 58, in main
    results = readfile(rank, fid+'.nc', setup['readpattern'], setup['buffersize'], setup['randcount'])
Traceback (most recent call last):
    results = readfile(rank, fid+'.nc', setup['readpattern'], setup['buffersize'], setup['randcount'])
  File "/home/users/mjones07/userbased-bench/readfile_nc.py", line 81, in readfile_1d
  File "/home/users/mjones07/userbased-bench/main.py", line 70, in <module>
  File "/home/users/mjones07/userbased-bench/readfile_nc.py", line 81, in readfile_1d
    results = seq_read_1d(mpirank, fid, num_elements)
  File "/home/users/mjones07/userbased-bench/readfile_nc.py", line 10, in seq_read_1d
    main()
    results = seq_read_1d(mpirank, fid, num_elements)
  File "/home/users/mjones07/userbased-bench/readfile_nc.py", line 10, in seq_read_1d
  File "/home/users/mjones07/userbased-bench/main.py", line 58, in main
    f = Dataset(fid,'r')
  File "netCDF4/_netCDF4.pyx", line 1875, in netCDF4._netCDF4.Dataset.__init__ (netCDF4/_netCDF4.c:13376)
    f = Dataset(fid,'r')
    results = readfile(rank, fid+'.nc', setup['readpattern'], setup['buffersize'], setup['randcount'])
  File "netCDF4/_netCDF4.pyx", line 1875, in netCDF4._netCDF4.Dataset.__init__ (netCDF4/_netCDF4.c:13376)
  File "/home/users/mjones07/userbased-bench/readfile_nc.py", line 81, in readfile_1d
    results = seq_read_1d(mpirank, fid, num_elements)
  File "/home/users/mjones07/userbased-bench/readfile_nc.py", line 10, in seq_read_1d
    f = Dataset(fid,'r')
  File "netCDF4/_netCDF4.pyx", line 1875, in netCDF4._netCDF4.Dataset.__init__ (netCDF4/_netCDF4.c:13376)
  File "netCDF4/_netCDF4.pyx", line 1581, in netCDF4._netCDF4._ensure_nc_success (netCDF4/_netCDF4.c:12162)
  File "netCDF4/_netCDF4.pyx", line 1581, in netCDF4._netCDF4._ensure_nc_success (netCDF4/_netCDF4.c:12162)
  File "netCDF4/_netCDF4.pyx", line 1581, in netCDF4._netCDF4._ensure_nc_success (netCDF4/_netCDF4.c:12162)
IOError: NetCDF: Unknown file format
IOError: NetCDF: Unknown file format
IOError: NetCDF: Unknown file format
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
0,256000000,256000000,1000000.0,256.0,sequential,0.19,0.188035964966,1361.44167977
Sender: LSF System <lsfadmin@host251.jc.rl.ac.uk>
Subject: Job 6715918: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Exited

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 17:16:50 2017.
Job was executed on host(s) <3*host251.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 17:37:10 2017.
                            <1*host190.jc.rl.ac.uk>
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 17:37:10 2017.
Terminated at Thu Oct  5 17:37:13 2017.
Results reported at Thu Oct  5 17:37:13 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1.59 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   3 sec.
    Turnaround time :                            1223 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 8191
  daemon proc 8194 on host 192.168.114.251
    rank 0:  proc 8202
    rank 1:  proc 8203
    rank 2:  proc 8204
  daemon proc 31613 on host 192.168.110.190
    rank 3:  proc 31641
main.py: Rank 0:2: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:1: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:3: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.114.251 -- ranks 0 - 2
Host 1 -- ip 192.168.110.190 -- ranks 3

 host | 0    1
======|===========
    0 : SHM  TCP
    1 : TCP  SHM

 Prot -  All Intra-node communication is: SHM
 Prot -  All Inter-node communication is: TCP

Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 74, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 55, in main
    create_files(setup, mpisize, mpirank)
NameError: global name 'mpirank' is not defined
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 74, in <module>
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 74, in <module>
Traceback (most recent call last):
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 74, in <module>
  File "/home/users/mjones07/userbased-bench/main.py", line 55, in main
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 55, in main
    main()
    create_files(setup, mpisize, mpirank)
  File "/home/users/mjones07/userbased-bench/main.py", line 55, in main
    create_files(setup, mpisize, mpirank)
NameError: global name 'mpirank' is not defined
NameError: global name 'mpirank' is not defined
    create_files(setup, mpisize, mpirank)
NameError: global name 'mpirank' is not defined
Sender: LSF System <lsfadmin@host266.jc.rl.ac.uk>
Subject: Job 6717499: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Done

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 18:03:21 2017.
Job was executed on host(s) <4*host266.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 18:03:21 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 18:03:21 2017.
Terminated at Thu Oct  5 18:03:32 2017.
Results reported at Thu Oct  5 18:03:32 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   7.03 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   11 sec.
    Turnaround time :                            11 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 13420
  daemon proc 13423 on host 192.168.114.166
    rank 0:  proc 13427
    rank 1:  proc 13428
    rank 2:  proc 13429
    rank 3:  proc 13430
main.py: Rank 0:2: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:1: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:3: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.114.166 -- ranks 0 - 3

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
1,256000000,256000000,1000000.0,256.0,sequential,0.3,1.9424469471,131.792531262
2,256000000,256000000,1000000.0,256.0,sequential,0.22,1.96074199677,130.562817761
3,256000000,256000000,1000000.0,256.0,sequential,0.23,1.960958004,130.548435753
0,256000000,256000000,1000000.0,256.0,sequential,0.2,1.96117091179,130.53426321
Sender: LSF System <lsfadmin@host266.jc.rl.ac.uk>
Subject: Job 6717602: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Done

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 18:05:09 2017.
Job was executed on host(s) <4*host266.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 18:05:10 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 18:05:10 2017.
Terminated at Thu Oct  5 18:05:14 2017.
Results reported at Thu Oct  5 18:05:14 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6.98 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   4 sec.
    Turnaround time :                            5 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 13918
  daemon proc 13921 on host 192.168.114.166
    rank 0:  proc 13925
    rank 1:  proc 13926
    rank 2:  proc 13927
    rank 3:  proc 13928
main.py: Rank 0:2: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:3: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:1: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.114.166 -- ranks 0 - 3

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
0,256000000,256000000,1000000.0,256.0,sequential,0.31,0.,30
3,256000000,256000000,1000000.0,256.0,sequential,0.26,0.,27
2,256000000,256000000,1000000.0,256.0,sequential,0.27,0.,28
1,256000000,256000000,1000000.0,256.0,sequential,0.25,0.,29
Sender: LSF System <lsfadmin@host130.jc.rl.ac.uk>
Subject: Job 6717607: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Done

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Thu Oct  5 18:07:25 2017.
Job was executed on host(s) <4*host130.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Thu Oct  5 18:14:49 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Thu Oct  5 18:14:49 2017.
Terminated at Thu Oct  5 18:14:55 2017.
Results reported at Thu Oct  5 18:14:55 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6.78 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   6 sec.
    Turnaround time :                            450 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 29377
  daemon proc 29380 on host 192.168.107.130
    rank 0:  proc 29384
    rank 1:  proc 29385
    rank 2:  proc 29386
    rank 3:  proc 29387
main.py: Rank 0:3: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:1: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:2: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.107.130 -- ranks 0 - 3

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
/group_workspaces/jasmin/hiresgw/vol1/mj07/test.nc
0,256000000,256000000,1000000.0,256.0,sequential,0.27,0.,299.037120711
3,256000000,256000000,1000000.0,256.0,sequential,0.21,0.,382.442394209
1,256000000,256000000,1000000.0,256.0,sequential,0.21,0.,388.086923458
2,256000000,256000000,1000000.0,256.0,sequential,0.19,0.,387.961143973
Sender: LSF System <lsfadmin@host133.jc.rl.ac.uk>
Subject: Job 6770288: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Exited

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct  6 10:29:41 2017.
Job was executed on host(s) <4*host133.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct  6 10:29:45 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct  6 10:29:45 2017.
Terminated at Fri Oct  6 10:29:45 2017.
Results reported at Fri Oct  6 10:29:45 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.56 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   0 sec.
    Turnaround time :                            4 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 84, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
      File "/home/users/mjones07/userbased-bench/main.py", line 84, in <module>
  File "/home/users/mjones07/userbased-bench/main.py", line 84, in <module>
main()
  File "/home/users/mjones07/userbased-bench/main.py", line 46, in main
    main()
    main()
      File "/home/users/mjones07/userbased-bench/main.py", line 46, in main
comm = MPI.COMM_WORLD
  File "/home/users/mjones07/userbased-bench/main.py", line 46, in main
Traceback (most recent call last):
NameError  File "/home/users/mjones07/userbased-bench/main.py", line 84, in <module>
:     global name 'MPI' is not defined    comm = MPI.COMM_WORLD

comm = MPI.COMM_WORLD
NameError    NameError: main()
: global name 'MPI' is not definedglobal name 'MPI' is not defined

  File "/home/users/mjones07/userbased-bench/main.py", line 46, in main
    comm = MPI.COMM_WORLD
NameError: global name 'MPI' is not defined
MPI Application rank 1 exited before MPI_Init() with status 1
mpirun: Broken pipe
mpirun: propagating signal 13
Sender: LSF System <lsfadmin@host133.jc.rl.ac.uk>
Subject: Job 6772405: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Done

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct  6 10:30:02 2017.
Job was executed on host(s) <4*host133.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct  6 10:30:05 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct  6 10:30:05 2017.
Terminated at Fri Oct  6 10:30:17 2017.
Results reported at Fri Oct  6 10:30:17 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   8.00 sec.
    Max Memory :                                 82.54 MB
    Average Memory :                             82.54 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   3793 MB
    Max Processes :                              10
    Max Threads :                                12
    Run time :                                   12 sec.
    Turnaround time :                            15 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 27106
  daemon proc 27109 on host 192.168.107.133
    rank 0:  proc 27113
    rank 1:  proc 27114
    rank 2:  proc 27115
    rank 3:  proc 27116
main.py: Rank 0:3: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:2: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:1: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.107.133 -- ranks 0 - 3

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

256000000,256000000,1000000,256,sequential,0.180000,1.925786,132.932735
256000000,256000000,1000000,256,sequential,0.180000,1.936738,132.181018
256000000,256000000,1000000,256,sequential,0.140000,0.820648,311.948606
256000000,256000000,1000000,256,sequential,0.080000,0.082361,3108.267262
Sender: LSF System <lsfadmin@host053.jc.rl.ac.uk>
Subject: Job 6782393: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Exited

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct  6 10:31:59 2017.
Job was executed on host(s) <4*host053.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct  6 10:32:06 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct  6 10:32:06 2017.
Terminated at Fri Oct  6 10:32:12 2017.
Results reported at Fri Oct  6 10:32:12 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   6.87 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   6 sec.
    Turnaround time :                            13 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 678
  daemon proc 681 on host 192.168.102.53
    rank 0:  proc 685
    rank 1:  proc 686
    rank 2:  proc 687
    rank 3:  proc 688
main.py: Rank 0:1: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:2: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:3: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.102.53 -- ranks 0 - 3

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 84, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 76, in main
    results =  rank+','+output.split('\n')[4]
TypeError: unsupported operand type(s) for +: 'int' and 'str'
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 84, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 76, in main
    results =  rank+','+output.split('\n')[4]
TypeError: unsupported operand type(s) for +: 'int' and 'str'
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 84, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 76, in main
    results =  rank+','+output.split('\n')[4]
TypeError: unsupported operand type(s) for +: 'int' and 'str'
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/main.py", line 84, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/main.py", line 76, in main
    results =  rank+','+output.split('\n')[4]
TypeError: unsupported operand type(s) for +: 'int' and 'str'
Sender: LSF System <lsfadmin@host053.jc.rl.ac.uk>
Subject: Job 6785016: <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> in cluster <lotus> Done

Job <mpirun.lotus /home/users/mjones07/userbased-bench/main.py config> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct  6 10:32:30 2017.
Job was executed on host(s) <4*host053.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct  6 10:32:36 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct  6 10:32:36 2017.
Terminated at Fri Oct  6 10:32:40 2017.
Results reported at Fri Oct  6 10:32:40 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lotus /home/users/mjones07/userbased-bench/main.py config
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   7.54 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   4 sec.
    Turnaround time :                            10 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
main.py: Rank 0:3: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:1: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
main.py: Rank 0:2: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
mpirun:  proc 844
  daemon proc 847 on host 192.168.102.53
    rank 0:  proc 851
    rank 1:  proc 852
    rank 2:  proc 853
    rank 3:  proc 854
main.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.102.53 -- ranks 0 - 3

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

1,256000000,256000000,1000000,256,sequential,0.070000,0.082111,3117.730876
2,256000000,256000000,1000000,256,sequential,0.080000,0.083936,3049.942814
3,256000000,256000000,1000000,256,sequential,0.080000,0.081536,3139.717425
0,256000000,256000000,1000000,256,sequential,0.070000,0.081339,3147.321703
