Sender: LSF System <lsfadmin@host202.jc.rl.ac.uk>
Subject: Job 4404956: <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configchdf> in cluster <lotus> Exited

Job <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configchdf> was submitted from host <jasmin-sci2-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct 27 10:12:13 2017.
Job was executed on host(s) <host202.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct 27 10:12:14 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct 27 10:12:14 2017.
Terminated at Fri Oct 27 10:12:15 2017.
Results reported at Fri Oct 27 10:12:15 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q par-multi
#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out
#BSUB -W 24:00
#BSUB -n 1
#BSUB -R "span[ptile=1]"
####BSUB -x
####BSUB -m ivybridge128G
####BSUB -U iortestj3

mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configchdf

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.29 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   1 sec.
    Turnaround time :                            2 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 25464
  daemon proc 25467 on host 192.168.113.202
    rank 0:  proc 25471
userbased_nc_bench.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.113.202 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 164, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 103, in main
    setup = get_setup(setup_config_file)
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 85, in get_setup
    for line in open(setup_config_file):
IOError: [Errno 2] No such file or directory: '/home/users/mjones07/userbased-bench/configchdf'
Sender: LSF System <lsfadmin@host202.jc.rl.ac.uk>
Subject: Job 4404963: <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> in cluster <lotus> Exited

Job <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> was submitted from host <jasmin-sci2-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct 27 10:12:44 2017.
Job was executed on host(s) <host202.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct 27 10:12:45 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct 27 10:12:45 2017.
Terminated at Fri Oct 27 10:12:46 2017.
Results reported at Fri Oct 27 10:12:46 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q par-multi
#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out
#BSUB -W 24:00
#BSUB -n 1
#BSUB -R "span[ptile=1]"
####BSUB -x
####BSUB -m ivybridge128G
####BSUB -U iortestj3

mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.29 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   1 sec.
    Turnaround time :                            2 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 25541
  daemon proc 25544 on host 192.168.113.202
    rank 0:  proc 25548
userbased_nc_bench.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.113.202 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 164, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 104, in main
    print setups
NameError: global name 'setups' is not defined
Sender: LSF System <lsfadmin@host202.jc.rl.ac.uk>
Subject: Job 4404966: <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> in cluster <lotus> Exited

Job <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> was submitted from host <jasmin-sci2-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct 27 10:12:56 2017.
Job was executed on host(s) <host202.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct 27 10:12:57 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct 27 10:12:57 2017.
Terminated at Fri Oct 27 10:12:58 2017.
Results reported at Fri Oct 27 10:12:58 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q par-multi
#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out
#BSUB -W 24:00
#BSUB -n 1
#BSUB -R "span[ptile=1]"
####BSUB -x
####BSUB -m ivybridge128G
####BSUB -U iortestj3

mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.37 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   1 sec.
    Turnaround time :                            2 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 25606
  daemon proc 25609 on host 192.168.113.202
    rank 0:  proc 25613
userbased_nc_bench.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.113.202 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

{'sharedfile': 0L, 'language': 'Python', 'readpattern': 's', 'filetype': 'hdf', 'randcount': 0L, 'mpiio': 0L, 'cleanup': 0L, 'filesize': 256000000000L, 'fname': 'test', 'floc': '/group_workspaces/jasmin/hiresgw/vol1/mj07/', 'output': '/home/users/mjones07/userbased-bench/', 'rundir': '/home/users/mjones07/userbased-bench/', 'buffersize': 1000000L}
mpi rank 0 creating file /group_workspaces/jasmin/hiresgw/vol1/mj07/test0.nc
write,0,256000000000,2.78949737549e-05,0.0,7780737855.07
/group_workspaces/jasmin/hiresgw/vol1/mj07/test0.nc
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 164, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 137, in main
    results = 'read,'+readfile(rank, fid+str(rank)+'.nc', setup['readpattern'], setup['buffersize'], setup['randcount'])
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/readfile_hdf.py", line 81, in readfile_1d
    results = seq_read_1d(mpirank, fid, num_elements)
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/readfile_hdf.py", line 11, in seq_read_1d
    var = f['data']
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper (/home/builderdev/rpmbuild/BUILD/h5py-2.7.1/h5py/_objects.c:2678)
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper (/home/builderdev/rpmbuild/BUILD/h5py-2.7.1/h5py/_objects.c:2636)
  File "/usr/lib/python2.7/site-packages/h5py/_hl/group.py", line 167, in __getitem__
    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper (/home/builderdev/rpmbuild/BUILD/h5py-2.7.1/h5py/_objects.c:2678)
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper (/home/builderdev/rpmbuild/BUILD/h5py-2.7.1/h5py/_objects.c:2636)
  File "h5py/h5o.pyx", line 190, in h5py.h5o.open (/home/builderdev/rpmbuild/BUILD/h5py-2.7.1/h5py/h5o.c:3569)
KeyError: "Unable to open object (object 'data' doesn't exist)"
Sender: LSF System <lsfadmin@host186.jc.rl.ac.uk>
Subject: Job 4407235: <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> in cluster <lotus> Exited

Job <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> was submitted from host <jasmin-sci2-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct 27 10:16:14 2017.
Job was executed on host(s) <host186.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct 27 10:16:15 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct 27 10:16:15 2017.
Terminated at Fri Oct 27 10:16:16 2017.
Results reported at Fri Oct 27 10:16:16 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q par-multi
#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out
#BSUB -W 24:00
#BSUB -n 1
#BSUB -R "span[ptile=1]"
####BSUB -x
####BSUB -m ivybridge128G
####BSUB -U iortestj3

mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.34 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   2 sec.
    Turnaround time :                            2 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 29472
  daemon proc 29475 on host 192.168.110.186
    rank 0:  proc 29479
userbased_nc_bench.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.110.186 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

{'sharedfile': 0L, 'language': 'Python', 'readpattern': 's', 'filetype': 'hdf', 'randcount': 0L, 'mpiio': 0L, 'cleanup': 0L, 'filesize': 256000000000L, 'fname': 'test', 'floc': '/group_workspaces/jasmin/hiresgw/vol1/mj07/', 'output': '/home/users/mjones07/userbased-bench/', 'rundir': '/home/users/mjones07/userbased-bench/', 'buffersize': 1000000L}
mpi rank 0 creating file /group_workspaces/jasmin/hiresgw/vol1/mj07/test0.nc
write,0,256000000000,2.47955322266e-05,0.0,9177280547.01
/group_workspaces/jasmin/hiresgw/vol1/mj07/test0.hdf5
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 164, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 137, in main
    results = 'read,'+readfile(rank, fid+str(rank)+'.hdf5', setup['readpattern'], setup['buffersize'], setup['randcount'])
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/readfile_hdf.py", line 81, in readfile_1d
    results = seq_read_1d(mpirank, fid, num_elements)
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/readfile_hdf.py", line 10, in seq_read_1d
    f = h5py.File(fid,'r')
  File "/usr/lib/python2.7/site-packages/h5py/_hl/files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "/usr/lib/python2.7/site-packages/h5py/_hl/files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper (/home/builderdev/rpmbuild/BUILD/h5py-2.7.1/h5py/_objects.c:2678)
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper (/home/builderdev/rpmbuild/BUILD/h5py-2.7.1/h5py/_objects.c:2636)
  File "h5py/h5f.pyx", line 78, in h5py.h5f.open (/home/builderdev/rpmbuild/BUILD/h5py-2.7.1/h5py/h5f.c:1959)
IOError: Unable to open file (unable to open file: name = '/group_workspaces/jasmin/hiresgw/vol1/mj07/test0.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
Sender: LSF System <lsfadmin@host073.jc.rl.ac.uk>
Subject: Job 4407521: <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> in cluster <lotus> Exited

Job <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> was submitted from host <jasmin-sci2-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct 27 10:18:27 2017.
Job was executed on host(s) <host073.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct 27 10:18:28 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct 27 10:18:28 2017.
Terminated at Fri Oct 27 10:18:30 2017.
Results reported at Fri Oct 27 10:18:30 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q par-multi
#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out
#BSUB -W 24:00
#BSUB -n 1
#BSUB -R "span[ptile=1]"
####BSUB -x
####BSUB -m ivybridge128G
####BSUB -U iortestj3

mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.27 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   2 sec.
    Turnaround time :                            3 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 1001
  daemon proc 1004 on host 192.168.102.73
    rank 0:  proc 1008
userbased_nc_bench.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.102.73 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

{'sharedfile': 0L, 'language': 'Python', 'readpattern': 's', 'filetype': 'hdf', 'randcount': 0L, 'mpiio': 0L, 'cleanup': 0L, 'filesize': 256000000000L, 'fname': 'test', 'floc': '/group_workspaces/jasmin/hiresgw/vol1/mj07/', 'output': '/home/users/mjones07/userbased-bench/', 'rundir': '/home/users/mjones07/userbased-bench/', 'buffersize': 1000000L}
mpi rank 0 creating file /group_workspaces/jasmin/hiresgw/vol1/mj07/test0.nc
Traceback (most recent call last):
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 168, in <module>
    main()
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 121, in main
    create_files(setup, mpisize, rank)
  File "/home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py", line 66, in create_files
    netcdf_creation.create_hdf5_1d(setup['filesize'], setup['floc'], setup['fname'], setup['buffersize'], mpisize-1, setup['stor'])
KeyError: 'stor'
Sender: LSF System <lsfadmin@host073.jc.rl.ac.uk>
Subject: Job 4407524: <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> in cluster <lotus> Done

Job <#!/bin/bash;#BSUB -q par-multi;#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out;#BSUB -W 24:00;#BSUB -n 1;#BSUB -R "span[ptile=1]";####BSUB -x;####BSUB -m ivybridge128G;####BSUB -U iortestj3; mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf> was submitted from host <jasmin-sci2-panfs.ceda.ac.uk> by user <mjones07> in cluster <lotus> at Fri Oct 27 10:19:02 2017.
Job was executed on host(s) <host073.jc.rl.ac.uk>, in queue <par-multi>, as user <mjones07> in cluster <lotus> at Fri Oct 27 10:19:02 2017.
</home/users/mjones07> was used as the home directory.
</home/users/mjones07/userbased-bench> was used as the working directory.
Started at Fri Oct 27 10:19:02 2017.
Terminated at Fri Oct 27 10:57:44 2017.
Results reported at Fri Oct 27 10:57:44 2017.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q par-multi
#BSUB -o /home/users/mjones07/userbased-bench/resultsn1c1.out
#BSUB -W 24:00
#BSUB -n 1
#BSUB -R "span[ptile=1]"
####BSUB -x
####BSUB -m ivybridge128G
####BSUB -U iortestj3

mpirun.lotus /home/users/mjones07/userbased-bench/userbased_nc_bench/userbased_nc_bench.py /home/users/mjones07/userbased-bench/configpyhdf

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   914.03 sec.
    Max Memory :                                 30.14 MB
    Average Memory :                             29.70 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   2420 MB
    Max Processes :                              8
    Max Threads :                                10
    Run time :                                   2322 sec.
    Turnaround time :                            2322 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 1094
  daemon proc 1097 on host 192.168.102.73
    rank 0:  proc 1101
userbased_nc_bench.py: Rank 0:0: MPI_Init_thread: Unable to open /opt/platform_mpi/lib/linux_amd64/libcoll.so: undefined symbol: hpmp_comm_world - running with failsafe mpi collective algorithms.
Host 0 -- ip 192.168.102.73 -- ranks 0

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

{'sharedfile': 0L, 'language': 'Python', 'readpattern': 's', 'filetype': 'hdf', 'randcount': 0L, 'mpiio': 0L, 'stor': 'filesystem', 'cleanup': 0L, 'filesize': 256000000000L, 'fname': 'test', 'floc': '/group_workspaces/jasmin/hiresgw/vol1/mj07/', 'output': '/home/users/mjones07/userbased-bench/', 'rundir': '/home/users/mjones07/userbased-bench/', 'buffersize': 1000000L}
mpi rank 0 creating file /group_workspaces/jasmin/hiresgw/vol1/mj07/test0.nc
write,0,256000000000,1315.56118298,717.22,194.593760719
/group_workspaces/jasmin/hiresgw/vol1/mj07/test0.hdf5
read,0,256000000000,256000000000,1000000.0,256000.0,sequential,189.5,1004.6330111,254.819418804
